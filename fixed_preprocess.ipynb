{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023d1b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy\n",
    "python -m spacy download nl_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0541c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "from multiprocessing import  Pool\n",
    "from functools import partial\n",
    "from project_variables import project_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3c5700",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(project_path)\n",
    "\n",
    " \n",
    "# Variables\n",
    "upwindow = 7\n",
    "lowwindow = 2\n",
    "\n",
    "'''\n",
    "Preprocessing fuction for both the children as the parents\n",
    "'''\n",
    "def preprocessing_parent(parents):\n",
    "    # Parents\n",
    "    parents.loc[:,'related_children'] = parents['related_children'].astype(str)\n",
    "    parents.loc[:,'title'] = parents['title'].astype(str)\n",
    "    parents.loc[:,'content'] = parents['content'].astype(str)\n",
    "    parents.loc[:,'publish_date'] = pd.to_datetime(parents.loc[:,'publish_date'])\n",
    "    parents.loc[:,'publish_date'] = parents['publish_date'].dt.tz_localize(None)\n",
    "    parents.loc[:,'title'] = parents.loc[:,'title'].str.lower()\n",
    "    parents.loc[:,'content'] = parents.loc[:,'content'].str.lower()\n",
    "    parents.loc[:,'content'] = parents.loc[:,'content'].str.replace('-',' ')\n",
    "    parents.loc[:,'content'] = parents.loc[:,'content'].str.replace('  ',' ')\n",
    "    parents.loc[:,'related_children'] = parents['related_children'].str.replace('matches/','').str.split(',')\n",
    "    \n",
    "    taxonomie_df = pd.read_csv(str('/Users/<USERNAME>/Documents/Programma/Final_CBS_mediakoppeling-master/Data/taxonomie_df.csv'),index_col=0)\n",
    "    taxonomie_df[taxonomie_df=='999'] = None\n",
    "    taxonomie_df[taxonomie_df=='999.0'] = None\n",
    "    parents.loc[:,'found_synonyms'] = parents.apply(find_synoniemen, args=(taxonomie_df,),axis=1)\n",
    "    parents.loc[:,'Gebruik_UF'] = [d[0] for d in parents['found_synonyms']]\n",
    "    parents.loc[:,'BT_TT'] = [d[1] for d in parents['found_synonyms']]\n",
    "    parents.drop(['found_synonyms'], axis=1)\n",
    "    \n",
    "    # select numbers from parent\n",
    "    parents.loc[:,'parent_numbers'] = parents.apply(regex,args=('content',),axis=1)\n",
    "    \n",
    "    parents.loc[:,'first_paragraph_without_stopwords'] = parents.apply(select_and_prepare_first_paragraph_of_CBS_article,axis=1)\n",
    "    parents.loc[:,'title_without_stopwords'] = parents.apply(select_and_prepare_title_of_CBS_article,axis=1)\n",
    "    \n",
    "    # remove numbers from parent\n",
    "    parents.loc[:,'content_no_numbers'] = parents.apply(remove_numbers,args=('content',),axis=1)    \n",
    "    # remove stopwords from content\n",
    "    parents.loc[:,'content_without_stopwords'] = parents.apply(remove_stopwords_from_content, args=('content_no_numbers',),axis=1)\n",
    "    return parents\n",
    "#%%\n",
    "def preprocessing_child(children):    \n",
    "    # Children\n",
    "    children.loc[:,'related_parents'] = children['related_parents'].astype(str)\n",
    "    children.loc[:,'title'] = children['title'].astype(str)\n",
    "    children.loc[:,'content'] = children['content'].astype(str)\n",
    "    children.loc[:,'title'] = children.loc[:,'title'].str.lower()\n",
    "    children.loc[:,'content'] = children.loc[:,'content'].str.lower()\n",
    "    children.loc[:,'publish_date'] = pd.to_datetime(children.loc[:,'publish_date'])\n",
    "    children.loc[:,'publish_date'] = children['publish_date'].dt.tz_localize(None)\n",
    "    \n",
    "    # replace other references to cbs with cbs itself\n",
    "    children.loc[:,'content'] = children.loc[:,'content'].str.replace('centraal bureau voor de statistiek','cbs')\n",
    "    children.loc[:,'content'] = children.loc[:,'content'].str.replace('cbs(cbs)','cbs')\n",
    "    children.loc[:,'content'] = children.loc[:,'content'].str.replace('cbs (cbs)','cbs')\n",
    "    children.loc[:,'content'] = children.loc[:,'content'].str.replace('cbs ( cbs )','cbs')\n",
    "    return children\n",
    "\n",
    "def find_link(row):\n",
    "    '''\n",
    "    # Function to check if there is a link to the CBS site\n",
    "    # children['cbs_link_in_child'] = children.apply(find_link,axis=1)\n",
    "    \n",
    "    Input: \n",
    "        - row with all data regarding the newsarticle (content is used)\n",
    "        - dataframe with all parents\n",
    "    Ouput: id(s) from parent article\n",
    "    '''\n",
    "    # select content from row\n",
    "    link=''\n",
    "    content = row['content']\n",
    "    if type(content) != float:\n",
    "        # some preprocessing of the content\n",
    "        content = content.replace('- ','-')\n",
    "        # split the content in words\n",
    "        splitted = content.split(' ')\n",
    "        \n",
    "        \n",
    "        # check the words for cbs site\n",
    "        for split in splitted:\n",
    "            if 'www.cbs.nl/' in split:\n",
    "                #link.append(split)\n",
    "                link=split\n",
    "                if type(link)==str:\n",
    "                    link = link.translate({ord(i):None for i in '()'})\n",
    "                    # puts them nicely in a list if any article has multiple links. \n",
    "    #                for id in parents[parents['link'].str.contains(link)==True]['id'].values:\n",
    "    #                    matches_to_return.append(id)\n",
    "    return link\n",
    "\n",
    "def find_id(row,df,level):\n",
    "    '''\n",
    "    Function to get Id back from the levels generated by the record linkage\n",
    "    '''\n",
    "    return df.loc[row[level],'id']\n",
    "\n",
    "def find_title(row):\n",
    "    '''\n",
    "    Check if whole title is in the content of the child article\n",
    "    '''\n",
    "    title = row['title_parent']\n",
    "    \n",
    "    try:\n",
    "        if (title in row['content_child'])&(type(title) != float):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "def find_sleutelwoorden_UF(row):\n",
    "    '''\n",
    "    Get jaccard score and number of matches based on the sleutelwoorden and highest taxonomy synomyms\n",
    "    '''\n",
    "    if type(row['content_child_no_stop']) == float:\n",
    "        return pd.Series([0,0,{''}])\n",
    "    content = re.sub(r'[^\\w\\s]','',row['content_child_no_stop'])                             # Remove punctuation\n",
    "    if type(row['taxonomies']) == float:\n",
    "        return pd.Series([0,0,{''}])\n",
    "    try:\n",
    "        taxonomies = row['taxonomies'].split(',')\n",
    "        # extend list of sleutelwoorden, or append, depending on the size of the synonyms. \n",
    "        if len(row['Gebruik_UF'].split(' '))>1:\n",
    "            taxonomies.extend(row['Gebruik_UF'].split(' '))\n",
    "        else:\n",
    "            taxonomies.append(row['Gebruik_UF'].split(' '))\n",
    "        matches = {x for x in taxonomies if x in content}\n",
    "        jaccard = len(matches)/len(list(set(taxonomies)))\n",
    "        return pd.Series([jaccard, len(matches),matches])\n",
    "    except:\n",
    "        return pd.Series([0,0,{''}])\n",
    "    \n",
    "def find_BT_TT(row):\n",
    "    '''\n",
    "    Get jaccard score and number of matches based on the Broader Terms and Top Terms of the sleutelwoorden    \n",
    "    '''\n",
    "    if type(row['content_child_no_stop']) == float:\n",
    "        return pd.Series([0,0,{''}])\n",
    "    content = re.sub(r'[^\\w\\s]','',row['content_child_no_stop'])                             # Remove punctuation\n",
    "    if type(row['BT_TT']) == float:\n",
    "        return pd.Series([0,0,{''}])\n",
    "    try:\n",
    "        taxonomies = row['BT_TT'].split(' ')\n",
    "        matches = {x for x in taxonomies if x in content}\n",
    "        jaccard = len(matches)/len(list(set(taxonomies)))\n",
    "        return pd.Series([jaccard, len(matches),matches])\n",
    "    except:\n",
    "        return pd.Series([0,0,{''}])\n",
    "\n",
    "def find_title_no_stop(row):\n",
    "    '''\n",
    "    Get jaccard score and number of matches based on the words in the introduction\n",
    "    '''\n",
    "    if type(row['content_child_no_stop']) == float:\n",
    "        return pd.Series([0,0,{''}])\n",
    "    content = re.sub(r'[^\\w\\s]','',row['content_child_no_stop'])                             # Remove punctuation\n",
    "    if type(row['title_without_stopwords']) == float:\n",
    "        return pd.Series([0,0,{''}])\n",
    "    try:\n",
    "        taxonomies = row['title_without_stopwords'].split(' ')\n",
    "        matches = {x for x in taxonomies if x in content}\n",
    "        jaccard = len(matches)/len(list(set(taxonomies)))\n",
    "        return pd.Series([jaccard, len(matches),matches])\n",
    "    except:\n",
    "        return pd.Series([0,0,{''}])\n",
    "    \n",
    "def find_1st_paragraph_no_stop(row):\n",
    "    '''\n",
    "    Get jaccard score and number of matches based on the words in the first paragraph of the parent\n",
    "    '''\n",
    "    if type(row['content_child_no_stop']) == float:\n",
    "        return pd.Series([0,0,{''}])\n",
    "    content = re.sub(r'[^\\w\\s]','',row['content_child_no_stop'])                             # Remove punctuation\n",
    "    content = re.sub(r'cbs','',row['content_child_no_stop'])                             # Remove cbs\n",
    "    if type(row['first_paragraph_without_stopwords']) == float:\n",
    "        return pd.Series([0,0,{''}])\n",
    "    try:\n",
    "        taxonomies = row['first_paragraph_without_stopwords'].split(' ')\n",
    "        matches = {x for x in taxonomies if x in content}\n",
    "        jaccard = len(matches)/len(list(set(taxonomies)))\n",
    "        return pd.Series([jaccard, len(matches),matches])\n",
    "    except:\n",
    "        return pd.Series([0,0,{''}])\n",
    "    \n",
    "def determine_matches(row):\n",
    "    '''\n",
    "    Check if records are a match\n",
    "    '''\n",
    "    if str(row['parent_id']) in row['related_parents']:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def date_comparison(row,offset,scale):  \n",
    "    '''\n",
    "    Compare the dates and return a score\n",
    "    '''  \n",
    "    diff = row['date_diff_days']\n",
    "    return 2**(-(diff-offset)/scale)\n",
    "\n",
    "def find_numbers(row):\n",
    "    '''\n",
    "    Get jaccard score and number of matches based on the words in the first paragraph of the parent\n",
    "    '''\n",
    "    content = ' '.join(row.loc['child_numbers'])                             \n",
    "    try:\n",
    "        taxonomies = row['parent_numbers']\n",
    "        matches = {x for x in taxonomies if x in content}\n",
    "        jaccard = len(matches)/len(list(set(taxonomies)))\n",
    "        return pd.Series([jaccard, len(matches),matches])\n",
    "    except:\n",
    "        return pd.Series([0,0,{''}])\n",
    "    \n",
    "def similarity(row,nlp):\n",
    "    try:\n",
    "        title_parent = nlp(row['title_without_stopwords'])\n",
    "        title_child = nlp(row['title_child_no_stop'])\n",
    "        content_parent = nlp(row['content_without_stopwords'])\n",
    "        content_child = nlp(row['content_child_no_stop'])\n",
    "        \n",
    "        if (title_parent.vector_norm == 0) | (title_child.vector_norm == 0):\n",
    "            title_similarity = 0\n",
    "        else:\n",
    "            title_similarity = title_parent.similarity(title_child)\n",
    "        if (content_parent.vector_norm == 0) | (content_child.vector_norm == 0):\n",
    "            content_similarity = 0\n",
    "        else:\n",
    "            content_similarity = content_parent.similarity(content_child)\n",
    "        return pd.Series([title_similarity, content_similarity])\n",
    "    except:\n",
    "        return pd.Series([0, 0])\n",
    "    \n",
    "def parallelize(data, func, num_of_processes=8):\n",
    "    data_split = np.array_split(data, num_of_processes)\n",
    "    pool = Pool(num_of_processes)\n",
    "    data = pd.concat(pool.map(func, data_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return data\n",
    "\n",
    "def run_on_subset(func, data_subset):\n",
    "    return data_subset.apply(func, axis=1)\n",
    "\n",
    "def parallelize_on_rows(data, func, num_of_processes=8):\n",
    "    return parallelize(data, partial(run_on_subset, func), num_of_processes)\n",
    "    \n",
    "def determine_vrijenieuwsgaring(row):\n",
    "    '''\n",
    "    Check if records are a match with vrijenieuwsgaring\n",
    "    '''\n",
    "    if '158123' in row['related_parents']:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def process_taxonomie_database():\n",
    "    '''\n",
    "    Function to process the cbs taxonomie database of Henk Laloli.\n",
    "    Input:\n",
    "        None, but uses the .txt database dump at the specified location\n",
    "    Output:\n",
    "        Writes pandas dataframe as csv at specified location. Word is on the index and the other terms in the columns\n",
    "    '''\n",
    "    libpath = Path('/Users/rwsla/Lars/CBS_2_mediakoppeling/data/taxonomie/')\n",
    "    \n",
    "    f = open(str(libpath / \"cbs-taxonomie-alfabetische-lijst.txt\"), \"r\",encoding='utf-8')\n",
    "    \n",
    "    lines = [line.rstrip('\\n') for line in f]\n",
    "    lines = lines[8:]\n",
    "    lines = filter(None, lines) # remove elements that contain of empty strings\n",
    "    df = pd.DataFrame()\n",
    "    for x in lines:\n",
    "        if not x.startswith('\t'):\n",
    "            index = x\n",
    "            for column in ['GEBRUIK','TT','UF','BT','RT','CBS English','NT','Historische notitie',\n",
    "                           'Scope notitie','Code','Eurovoc','DF','EQ']:\n",
    "                df.loc[index,column] = 999\n",
    "        if x.startswith('\t'):\n",
    "            column = x.split(':')[0][1:] # skip first two letters '/t'\n",
    "            value = x.split(':')[1][1:] # second part is the word, starts with space\n",
    "            if df.loc[index,column] == 999:\n",
    "                df.loc[index,column] = value\n",
    "            else:\n",
    "                df.loc[index,column] = value+', '+str(df.loc[index,column])\n",
    "            \n",
    "    f.close()\n",
    "    df = df[['GEBRUIK','TT','UF','BT','RT','CBS English','NT','Historische notitie','Scope notitie']]\n",
    "    df[df==999] = None\n",
    "    df.to_csv(str(libpath / 'taxonomie_df.csv'))\n",
    "    \n",
    "    '''\n",
    "    TT = TopTerm\n",
    "    UF = Gebruikt voor\n",
    "    BT = BredereTerm\n",
    "    RT = RelatedTerm\n",
    "    NT = NauwereTerm\n",
    "    '''\n",
    "    \n",
    "def find_synoniemen(row,taxonomie_df):\n",
    "    '''\n",
    "    sleutelwoorden(taxonomie) aanvullen met synoniemen op basis van de taxonomie database van Henk Laloli:\n",
    "        2 kolommen, een met de Gebruik kolom en de UsedFor kolom uit de database en een met de BredereTerm en de TopTerm.\n",
    "        De resultaten van de laatste kolom moeten in mindere mate meewerken aan matching score. \n",
    "    '''\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    \n",
    "    stop_words = set(stopwords.words('dutch'))\n",
    "    taxonomies = row['taxonomies']\n",
    "    Gebruik_UF = ''\n",
    "    BT_TT = ''\n",
    "    if type(taxonomies) != float:     \n",
    "        taxonomies = taxonomies.split(',')                                     # Some parents have no content (nan)\n",
    "        for taxonomie in taxonomies:\n",
    "            if taxonomie in taxonomie_df.index:\n",
    "                if taxonomie_df.loc[taxonomie,'GEBRUIK'] != None:\n",
    "                    Gebruik_UF = Gebruik_UF + ' ' + taxonomie_df.loc[taxonomie,'GEBRUIK']\n",
    "                if taxonomie_df.loc[taxonomie,'UF'] != None:\n",
    "                    Gebruik_UF = Gebruik_UF + ' ' + taxonomie_df.loc[taxonomie,'UF']\n",
    "                if taxonomie_df.loc[taxonomie,'TT'] != None:\n",
    "                    BT_TT = BT_TT + ' ' + taxonomie_df.loc[taxonomie,'TT']\n",
    "                if taxonomie_df.loc[taxonomie,'BT'] != None:\n",
    "                    BT_TT = BT_TT + ' ' + taxonomie_df.loc[taxonomie,'BT']\n",
    "    \n",
    "    temp = nltk.tokenize.word_tokenize(Gebruik_UF)\n",
    "    Gebruik_UF = [w for w in temp if not w in stop_words]\n",
    "    temp = nltk.tokenize.word_tokenize(BT_TT)\n",
    "    BT_TT = [w for w in temp if not w in stop_words]\n",
    "    return (' '.join(Gebruik_UF),' '.join(BT_TT))\n",
    "\n",
    "def select_and_prepare_first_paragraph_of_CBS_article(row, column = 'content'):\n",
    "    '''\n",
    "    Function to find the first paragraph of the CBS article, remove stopwords and return it as a string.\n",
    "    '''\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    import re\n",
    "    stop_words = set(stopwords.words('dutch'))\n",
    "    \n",
    "    filtered_intro = ''                                                 # Set as empty string for rows without content\n",
    "    content = row[column]\n",
    "    if type(content) != float:                                          # Some parents have no content (nan)\n",
    "        intro = content.split('\\n')[0]                                  # Select first block of text\n",
    "        intro = re.sub(r'[^\\w\\s]','',intro)                             # Remove punctuation\n",
    "        intro = nltk.tokenize.word_tokenize(intro)\n",
    "        filtered_intro = [w for w in intro if not w in stop_words]      # Remove stopwords\n",
    "    return ' '.join(filtered_intro)                                     # Convert from list to space-seperated string\n",
    "\n",
    "def select_and_prepare_title_of_CBS_article(row, column = 'title'):\n",
    "    '''\n",
    "    Function to remove stopwords from the title and return it as a string.\n",
    "    '''\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    import re\n",
    "    stop_words = set(stopwords.words('dutch'))\n",
    "    \n",
    "    filtered_title = ''                                                 # Set as empty string for rows without content\n",
    "    title = row[column]\n",
    "    if type(title) != float:                                          # Some parents have no content (nan)\n",
    "        title = re.sub(r'[^\\w\\s]','',title)                             # Remove punctuation\n",
    "        title = nltk.tokenize.word_tokenize(title)\n",
    "        filtered_title = [w for w in title if not w in stop_words]      # Remove stopwords\n",
    "    return ' '.join(filtered_title)                                     # Convert from list to space-seperated string\n",
    "\n",
    "def remove_stopwords_from_content(row, column = 'content'):\n",
    "    '''\n",
    "    Function to remove stopwords from the content and return it as a string.\n",
    "    '''\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    import re\n",
    "    stop_words = set(stopwords.words('dutch'))\n",
    "    \n",
    "    filtered_content = ''                                                 # Set as empty string for rows without content\n",
    "    content = row[column]\n",
    "    if type(content) != float:                                          # Some parents have no content (nan)\n",
    "        content = re.sub(r'[^\\w\\s]','',content)                             # Remove punctuation\n",
    "        content = nltk.tokenize.word_tokenize(content)\n",
    "        filtered_content = [w for w in content if not w in stop_words]      # Remove stopwords\n",
    "    return ' '.join(filtered_content)                                     # Convert from list to space-seperated string\n",
    "\n",
    "def regex(row, column = 'content'):\n",
    "    import re\n",
    "    \n",
    "    matches_to_return = []\n",
    "    if type(row[column]) != float:\n",
    "   \n",
    "        regex = r\"\\b(nul)\\b|\\b([a-zA-Z]*(twin|der|veer|vijf|zes|zeven|acht|negen)tig|[a-zA-Z]*tien|twee|drie|vier|vijf|zes|zeven|acht|negen|elf|twaalf)( )?(honderd|duizend|miljoen|miljard|procent)?\\b|\\b(honderd|duizend|miljoen|miljard)\\b|\\b[-+]?[.|,]?[\\d]+(?:,\\d\\d\\d)*[\\.|,]?\\d*([.|,]\\d+)*(?:[eE][-+]?\\d+)?( )?(honderd|duizend|miljoen|miljard|procent|%)?|half (miljoen|miljard|procent)\"\n",
    "        matches = re.finditer(regex, row[column])\n",
    "        \n",
    "        for matchNum, match in enumerate(matches, start=1):\n",
    "            string = match.group().strip().strip('.')\n",
    "            string = re.sub('%',' procent',string)\n",
    "            if re.match(r\"(\\d{1,3}[.]){1,3}\\d{3}\",string):\n",
    "                string= string.replace('.','')\n",
    "            else:\n",
    "                string= string.replace(',','.')\n",
    "            \n",
    "            if string.endswith(('honderd','duizend','miljoen','miljard','procent')):\n",
    "                endstring = re.search(r'honderd|duizend|miljoen|miljard|procent',string).group()\n",
    "                if endstring=='honderd':\n",
    "                    endstringmultiplier = 100\n",
    "                elif endstring=='duizend':\n",
    "                    endstringmultiplier = 1000\n",
    "                elif endstring=='miljoen':\n",
    "                    endstringmultiplier = 1000000\n",
    "                elif endstring=='miljard':\n",
    "                    endstringmultiplier = 1000000000\n",
    "                elif endstring=='procent':\n",
    "                    endstringmultiplier = 1\n",
    "                else:\n",
    "                    endstringmultiplier = 1\n",
    "                \n",
    "                # remove endstring from string\n",
    "                string = re.sub('honderd|duizend|miljoen|miljard|procent',  '',string)\n",
    "                # if empty, only endstring was string, example honderd\n",
    "                if re.match(r\"(\\d{1,3}[.]){1,3}\\d{3}\",string):\n",
    "                    string= string.replace('.','')\n",
    "                else:\n",
    "                    string= string.replace(',','.')\n",
    "                if string == '':\n",
    "                    matches_to_return.append(str(endstringmultiplier)) \n",
    "                else:\n",
    "                    try:\n",
    "                        string = own_word2num(string.strip('.').strip())# strip points and spaces in around match\n",
    "                        if endstring=='procent':\n",
    "                            matches_to_return.append(str(string)+' procent')\n",
    "                        else:\n",
    "                            matches_to_return.append(str(float(string)*endstringmultiplier)) \n",
    "                    except:\n",
    "                        try:\n",
    "                            string = string.strip('.').strip()\n",
    "                            if endstring=='procent':\n",
    "                                matches_to_return.append(str(string)+' procent')\n",
    "                            else:\n",
    "                                matches_to_return.append(str(float(string)*endstringmultiplier))\n",
    "                        except:\n",
    "                            pass\n",
    "            else:\n",
    "                try:\n",
    "                    matches_to_return.append(str(own_word2num(string))) \n",
    "                except:\n",
    "                    matches_to_return.append(str(string))\n",
    "    return list(set(matches_to_return))\n",
    "\n",
    "def remove_numbers(row, column = 'content'):\n",
    "    import re\n",
    "    import numpy as np\n",
    "    \n",
    "    if type(row[column]) != float:\n",
    "        regex = r\"\\b(nul)\\b|\\b([a-zA-Z]*(twin|der|veer|vijf|zes|zeven|acht|negen)tig|[a-zA-Z]*tien|twee|drie|vier|vijf|zes|zeven|acht|negen|elf|twaalf)( )?(honderd|duizend|miljoen|miljard|procent)?\\b|\\b(honderd|duizend|miljoen|miljard)\\b|\\b[-+]?[.|,]?[\\d]+(?:,\\d\\d\\d)*[\\.|,]?\\d*([.|,]\\d+)*(?:[eE][-+]?\\d+)?( )?(honderd|duizend|miljoen|miljard|procent|%)?|half (miljoen|miljard|procent)\"\n",
    "        return re.sub(regex,'',row[column])\n",
    "    else:\n",
    "        return(np.nan)\n",
    "        \n",
    "def own_word2num(string):\n",
    "    getal_dictionary = {\n",
    "        'nul': 0,\n",
    "        'half':0.5,\n",
    "        'een': 1,\n",
    "        'twee': 2,\n",
    "        'drie': 3,\n",
    "        'vier': 4,\n",
    "        'vijf': 5,\n",
    "        'zes': 6,\n",
    "        'zeven': 7,\n",
    "        'acht': 8,\n",
    "        'negen': 9,\n",
    "        'tien': 10,\n",
    "        'elf': 11,\n",
    "        'twaalf': 12,\n",
    "        'dertien': 13,\n",
    "        'veertien': 14,\n",
    "        'vijftien': 15,\n",
    "        'zestien': 16,\n",
    "        'zeventien': 17,\n",
    "        'achttien': 18,\n",
    "        'negentien': 19,\n",
    "        'twintig': 20,\n",
    "        'eenentwintig': 21,\n",
    "        'tweeentwintig': 22,\n",
    "        'drieentwintig': 23,\n",
    "        'vierentwintig': 24,\n",
    "        'vijfentwintig': 25,\n",
    "        'zesentwintig': 26,\n",
    "        'zevenentwintig': 27,\n",
    "        'achtentwintig': 28,\n",
    "        'negenentwintig': 29,\n",
    "        'dertig': 30,\n",
    "        'eenendertig': 31,\n",
    "        'tweeendertig': 32,\n",
    "        'drieendertig': 33,\n",
    "        'vierendertig': 34,\n",
    "        'vijfendertig': 35,\n",
    "        'zesendertig': 36,\n",
    "        'zevenendertig': 37,\n",
    "        'achtendertig': 38,\n",
    "        'negenendertig': 39,\n",
    "        'veertig': 40,\n",
    "        'eenenveertig': 41,\n",
    "        'tweeenveertig': 42,\n",
    "        'drieenveertig': 43,\n",
    "        'vierenveertig': 44,\n",
    "        'vijfenveertig': 45,\n",
    "        'zesenveertig': 46,\n",
    "        'zevenenveertig': 47,\n",
    "        'achtenveertig': 48,\n",
    "        'negenenveertig': 49,\n",
    "        'vijftig': 50,\n",
    "        'eenenvijftig': 51,\n",
    "        'tweeenvijftig': 52,\n",
    "        'drieenvijftig': 53,\n",
    "        'vierenvijftig': 54,\n",
    "        'vijfenvijftig': 55,\n",
    "        'zesenvijftig': 56,\n",
    "        'zevenenvijftig': 57,\n",
    "        'achtenvijftig': 58,\n",
    "        'negenenvijftig': 59,\n",
    "        'zestig': 60,\n",
    "        'eenenzestig': 61,\n",
    "        'tweeenzestig': 62,\n",
    "        'drieenzestig': 63,\n",
    "        'vierenzestig': 64,\n",
    "        'vijfenzestig': 65,\n",
    "        'zesenzestig': 66,\n",
    "        'zevenenzestig': 67,\n",
    "        'achtenzestig': 68,\n",
    "        'negenenzestig': 69,\n",
    "        'zeventig': 70,\n",
    "        'eenenzeventig': 71,\n",
    "        'tweeenzeventig': 72,\n",
    "        'drieenzeventig': 73,\n",
    "        'vierenzeventig': 74,\n",
    "        'vijfenzeventig': 75,\n",
    "        'zesenzeventig': 76,\n",
    "        'zevenenzeventig': 77,\n",
    "        'achtenzeventig': 78,\n",
    "        'negenenzeventig': 79,\n",
    "        'tachtig': 80,\n",
    "        'eenentachtig': 81,\n",
    "        'tweeentachtig': 82,\n",
    "        'drieentachtig': 83,\n",
    "        'vierentachtig': 84,\n",
    "        'vijfentachtig': 85,\n",
    "        'zesentachtig': 86,\n",
    "        'zevenentachtig': 87,\n",
    "        'achtentachtig': 88,\n",
    "        'negenentachtig': 89,\n",
    "        'negentig': 90,\n",
    "        'eenennegentig': 91,\n",
    "        'tweeennegentig': 92,\n",
    "        'drieennegentig': 93,\n",
    "        'vierennegentig': 94,\n",
    "        'vijfennegentig': 95,\n",
    "        'zesennegentig': 96,\n",
    "        'zevenennegentig': 97,\n",
    "        'achtennegentig': 98,\n",
    "        'negenennegentig': 99,\n",
    "        'honderd': 100,\n",
    "        'duizend': 1000,\n",
    "        'miljoen': 1000000,\n",
    "        'miljard': 1000000000,\n",
    "        'punt': '.'\n",
    "    }\n",
    "\n",
    "    return(getal_dictionary[string])\n",
    "    \n",
    "def keep_only_words(row,column):\n",
    "    row[column] = re.sub(r'set()','',row[column])\n",
    "    if column != 'numbers_matches':\n",
    "        row[column] = re.sub(r'[^\\w\\s]','',row[column])\n",
    "        row[column] = row[column].split(' ')\n",
    "    if column == 'numbers_matches':\n",
    "        row[column] = re.sub(r\"[{}'()]\",'',row[column])\n",
    "        row[column] = row[column].split(',')\n",
    "        row[column] = [x.strip(' ') for x in row[column]]\n",
    "    return row[column]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
