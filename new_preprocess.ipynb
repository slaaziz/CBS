{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d4f849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cb4971",
   "metadata": {},
   "source": [
    "# File Stitcher - RUN THIS CODE ONCE!\n",
    "\n",
    "There are 350k files in the data folder, which takes forever for the computer to go through. Also, there is \"all parents\" csv but some of the individual parent files are not included. So for convenience and having a complete set we will first stitch all the raw data into a single file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b3f10a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for c_*.csv files.\n",
      "Found 174190 files. Merging them into full_children.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174190/174190 [04:30<00:00, 644.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Saved full_children.csv with 174190 rows.\n",
      "Loading existing all_parents.csv to merge.\n",
      "Found 1594 individual parent files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1594/1594 [00:02<00:00, 596.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Saved full_parents.csv. Total parents: 2656\n"
     ]
    }
   ],
   "source": [
    "# DATA DIR\n",
    "data_dir = Path(\"data\")\n",
    "\n",
    "def build_master_csv(file_pattern, output_name, id_column='id'):\n",
    "\n",
    "    print(f\"Looking for {file_pattern} files.\")\n",
    "    # Get list of all files matching the pattern\n",
    "    # For children, we exclude the '_output.csv' files which are legacy matches\n",
    "    all_files = glob.glob(str(data_dir / file_pattern))\n",
    "    \n",
    "    # Filter out 'output' files if we are looking for children\n",
    "    if 'c_' in file_pattern:\n",
    "        all_files = [f for f in all_files if '_output.csv' not in f]\n",
    "    \n",
    "    print(f\"Found {len(all_files)} files. Merging them into {output_name}.\")\n",
    "    \n",
    "    # Use a list to store dataframes (faster than appending to a dataframe)\n",
    "    df_list = []\n",
    "    \n",
    "    for filename in tqdm(all_files):\n",
    "        try:\n",
    "            # Read only the columns we need to save memory\n",
    "            # Adjust columns based on what your new_preprocess.ipynb needs\n",
    "            df = pd.read_csv(filename, index_col=None, header=0)\n",
    "            df_list.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {filename}: {e}\")\n",
    "            continue\n",
    "            \n",
    "    if df_list:\n",
    "        master_df = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "        \n",
    "        # Deduplicate just in case\n",
    "        if id_column in master_df.columns:\n",
    "            master_df = master_df.drop_duplicates(subset=[id_column])\n",
    "            master_df = master_df.set_index(id_column) # Set ID as index for faster lookups\n",
    "            \n",
    "        # Save to CSV\n",
    "        output_path = data_dir / output_name\n",
    "        master_df.to_csv(output_path)\n",
    "        print(f\"Success! Saved {output_name} with {len(master_df)} rows.\")\n",
    "        return master_df\n",
    "    else:\n",
    "        print(f\"No files found for {file_pattern}!\")\n",
    "        return None\n",
    "\n",
    "# Build full_children.csv (formerly childs_from_id.csv)\n",
    "build_master_csv(\"c_*.csv\", \"full_children.csv\")\n",
    "\n",
    "# Build full_parents.csv (merges individual p_ files AND the original all_parents.csv input)\n",
    "existing_parents = []\n",
    "if (data_dir / \"all_parents.csv\").exists():\n",
    "    print(\"Loading existing all_parents.csv to merge.\")\n",
    "    existing_parents.append(pd.read_csv(data_dir / \"all_parents.csv\"))\n",
    "\n",
    "# Get all p_*.csv files\n",
    "p_files = glob.glob(str(data_dir / \"p_*.csv\"))\n",
    "print(f\"Found {len(p_files)} individual parent files.\")\n",
    "\n",
    "for f in tqdm(p_files):\n",
    "    existing_parents.append(pd.read_csv(f))\n",
    "\n",
    "if existing_parents:\n",
    "    full_parents = pd.concat(existing_parents, axis=0, ignore_index=True)\n",
    "    # Deduplicate by ID\n",
    "    if 'id' in full_parents.columns:\n",
    "        full_parents = full_parents.drop_duplicates(subset=['id'])\n",
    "        full_parents = full_parents.set_index('id')\n",
    "    \n",
    "    # Save to NEW name\n",
    "    full_parents.to_csv(data_dir / \"full_parents.csv\")\n",
    "    print(f\"Success! Saved full_parents.csv. Total parents: {len(full_parents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81270fe0",
   "metadata": {},
   "source": [
    "Carry on with rest of the processing:\n",
    "# SAFE TO RUN FROM THIS CELL AND DOWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47bce286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Model loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load the large Dutch model (For Model B)\n",
    "# If this crashes, Step 1 didn't work.\n",
    "nlp = spacy.load(\"nl_core_news_lg\", disable=['ner', 'parser'])\n",
    "print(\"Success: Model loaded!\")\n",
    "\n",
    "# Define paths\n",
    "data_path = Path(\"data\")\n",
    "\n",
    "# The English Stemmer for Model A (Make sure you imported nltk!)\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ee947a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download nl_core_news_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e455fa2c",
   "metadata": {},
   "source": [
    "Added Dutch SpaCy Lemmatization to PorterStemmer to actually capture Dutch meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9efb243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dutch cleaning (Model B)\n",
    "def clean_text_dutch(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text) \n",
    "    doc = nlp(text)\n",
    "    # Keep lemmas (Dutch dictionary roots)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Legacy English cleaning (Model A)\n",
    "def clean_text_legacy(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = text.lower()\n",
    "\n",
    "    # Simple regex\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    stems = [ps.stem(t) for t in tokens]\n",
    "    return \" \".join(stems)\n",
    "\n",
    "# CALculating the similarity score:\n",
    "def jaccard_similarity(str1, str2):\n",
    "    \"\"\"Legacy metric: Intersection over Union\"\"\"\n",
    "    a = set(str1.split()) \n",
    "    b = set(str2.split())\n",
    "    if len(a) == 0 or len(b) == 0: return 0.0\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fa2fc1",
   "metadata": {},
   "source": [
    "Cleaning the dataframes before the matching loop so we don't clean the same text 10,000 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0a94ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw CSVs, good luck to our laptops.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r5/_05_w2k14p728pkkf57ysjg40000gn/T/ipykernel_83559/1321362252.py:2: DtypeWarning: Columns (15,16,56,57,61,62,68,69,72) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  children_df = pd.read_csv(data_path / 'full_children.csv', index_col=0).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded and cleaned.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading raw CSVs, good luck to our laptops.\")\n",
    "children_df = pd.read_csv(data_path / 'full_children.csv', index_col=0).fillna('')\n",
    "parents_df = pd.read_csv(data_path / 'full_parents.csv', index_col=0).fillna('')\n",
    "\n",
    "# Force indices to be integers to ensure matching works\n",
    "children_df.index = pd.to_numeric(children_df.index, errors='coerce').fillna(-1).astype(int)\n",
    "parents_df.index = pd.to_numeric(parents_df.index, errors='coerce').fillna(-1).astype(int)\n",
    "\n",
    "# Model B / Dutch\n",
    "children_df['clean_title_dutch'] = children_df['title'].astype(str).apply(clean_text_dutch)\n",
    "children_df['clean_content_dutch'] = children_df['content'].astype(str).apply(clean_text_dutch)\n",
    "parents_df['clean_title_dutch'] = parents_df['title'].astype(str).apply(clean_text_dutch)\n",
    "parents_df['clean_content_dutch'] = parents_df['content'].astype(str).apply(clean_text_dutch)\n",
    "\n",
    "# Model A / English\n",
    "children_df['clean_title_legacy'] = children_df['title'].astype(str).apply(clean_text_legacy)\n",
    "children_df['clean_content_legacy'] = children_df['content'].astype(str).apply(clean_text_legacy)\n",
    "parents_df['clean_title_legacy'] = parents_df['title'].astype(str).apply(clean_text_legacy)\n",
    "parents_df['clean_content_legacy'] = parents_df['content'].astype(str).apply(clean_text_legacy)\n",
    "\n",
    "print(\"Data loaded and cleaned.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a462b62",
   "metadata": {},
   "source": [
    "The old script had issues with label overwriting. I wrote this loop to explicitly extract matches from the 'related_children' column and generate exactly 1 random negative for every positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e853e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building training pairs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2656/2656 [00:00<00:00, 28336.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 10 rows (Balanced 50/50).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "training_rows = []\n",
    "\n",
    "# List of all available parent IDs for random sampling\n",
    "all_parent_ids = parents_df.index.tolist()\n",
    "\n",
    "print(\"Building training pairs...\")\n",
    "\n",
    "for parent_id, row in tqdm(parents_df.iterrows(), total=len(parents_df)):\n",
    "    \n",
    "    # --- THE FIX IS HERE ---\n",
    "    # Use Regex to extract ALL numbers (IDs) from the messy string.\n",
    "    # This works for \"123, 456\", \"['123', '456']\", and \"matches/123\" all at once.\n",
    "    child_ids = re.findall(r'\\d+', str(row['related_children']))\n",
    "    \n",
    "    # If no IDs found, skip\n",
    "    if not child_ids:\n",
    "        continue\n",
    "        \n",
    "    for child_id in child_ids:\n",
    "        try:\n",
    "            c_id = int(child_id) # Convert extracted string to integer\n",
    "            \n",
    "            # Check if this child actually exists in our child database\n",
    "            if c_id in children_df.index:\n",
    "                \n",
    "                # --- CREATE POSITIVE SAMPLE (Match = 1) ---\n",
    "                training_rows.append({\n",
    "                    'child_id': c_id,\n",
    "                    'parent_id': parent_id,\n",
    "                    'match': 1\n",
    "                })\n",
    "                \n",
    "                # --- CREATE NEGATIVE SAMPLE (Match = 0) ---\n",
    "                # Pick a random parent that is NOT the current parent\n",
    "                while True:\n",
    "                    random_pid = random.choice(all_parent_ids)\n",
    "                    if random_pid != parent_id:\n",
    "                        break\n",
    "                \n",
    "                training_rows.append({\n",
    "                    'child_id': c_id,\n",
    "                    'parent_id': random_pid,\n",
    "                    'match': 0\n",
    "                })\n",
    "                \n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "# Convert to DataFrame\n",
    "trainset_new = pd.DataFrame(training_rows)\n",
    "print(f\"Generated {len(trainset_new)} rows (Balanced 50/50).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d77f8df",
   "metadata": {},
   "source": [
    "Using nlp.pipe to vectorize everything in batches. This is much faster than .iterrows()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "404ccf66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Vectors.\n",
      "Calculating Legacy Jaccard.\n",
      "Scoring complete.\n"
     ]
    }
   ],
   "source": [
    "# MOdel B\n",
    "# Map IDs to Dutch text\n",
    "c_titles_d = [children_df.at[cid, 'clean_title_dutch'] if cid in children_df.index else \"\" for cid in trainset_new['child_id']]\n",
    "p_titles_d = [parents_df.at[pid, 'clean_title_dutch'] if pid in parents_df.index else \"\" for pid in trainset_new['parent_id']]\n",
    "c_content_d = [children_df.at[cid, 'clean_content_dutch'] if cid in children_df.index else \"\" for cid in trainset_new['child_id']]\n",
    "p_content_d = [parents_df.at[pid, 'clean_content_dutch'] if pid in parents_df.index else \"\" for pid in trainset_new['parent_id']]\n",
    "\n",
    "print(\"Calculating Vectors.\")\n",
    "\n",
    "# Batch process with SpaCy\n",
    "c_title_docs = list(nlp.pipe(c_titles_d, batch_size=200))\n",
    "p_title_docs = list(nlp.pipe(p_titles_d, batch_size=200))\n",
    "c_cont_docs = list(nlp.pipe(c_content_d, batch_size=50))\n",
    "p_cont_docs = list(nlp.pipe(p_content_d, batch_size=50))\n",
    "\n",
    "trainset_new['title_sim_dutch'] = [c.similarity(p) if c.vector_norm and p.vector_norm else 0.0 for c, p in zip(c_title_docs, p_title_docs)]\n",
    "trainset_new['content_sim_dutch'] = [c.similarity(p) if c.vector_norm and p.vector_norm else 0.0 for c, p in zip(c_cont_docs, p_cont_docs)]\n",
    "\n",
    "# For model A\n",
    "print(\"Calculating Legacy Jaccard.\")\n",
    "\n",
    "# Map IDs to Legacy text\n",
    "c_titles_l = [children_df.at[cid, 'clean_title_legacy'] if cid in children_df.index else \"\" for cid in trainset_new['child_id']]\n",
    "p_titles_l = [parents_df.at[pid, 'clean_title_legacy'] if pid in parents_df.index else \"\" for pid in trainset_new['parent_id']]\n",
    "c_content_l = [children_df.at[cid, 'clean_content_legacy'] if cid in children_df.index else \"\" for cid in trainset_new['child_id']]\n",
    "p_content_l = [parents_df.at[pid, 'clean_content_legacy'] if pid in parents_df.index else \"\" for pid in trainset_new['parent_id']]\n",
    "\n",
    "# Simple list comprehension for Jaccard\n",
    "trainset_new['title_sim_legacy'] = [jaccard_similarity(c, p) for c, p in zip(c_titles_l, p_titles_l)]\n",
    "trainset_new['content_sim_legacy'] = [jaccard_similarity(c, p) for c, p in zip(c_content_l, p_content_l)]\n",
    "\n",
    "print(\"Scoring complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc41a593",
   "metadata": {},
   "source": [
    "Re-implementing the date binary check. I am omitting the complex taxonomy features for now as they require the legacy dictionary logic, but we have the core semantic features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1dac9661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   child_id  parent_id  match  title_sim_dutch  content_sim_dutch  \\\n",
      "0    955090     954939      1         1.000000           0.985670   \n",
      "1    955090    1458994      0         0.575754           0.718978   \n",
      "2    955140     954939      1         0.874414           0.662531   \n",
      "3    955140    1379022      0         0.410175           0.843156   \n",
      "4    955703     954939      1         0.668406           0.876471   \n",
      "\n",
      "   title_sim_legacy  content_sim_legacy  days_diff  date_binary  \n",
      "0          1.000000            0.735294   0.036921            1  \n",
      "1          0.000000            0.051546        NaN            0  \n",
      "2          0.555556            0.112108   0.590278            1  \n",
      "3          0.071429            0.135802        NaN            0  \n",
      "4          0.062500            0.151899   0.773819            1  \n",
      "Saved reconstructed training set to trainset_reconstructed.csv\n"
     ]
    }
   ],
   "source": [
    "# Add Date Logic\n",
    "# (We need to look up dates from the original dfs)\n",
    "c_dates = children_df.loc[trainset_new['child_id']]['publish_date']\n",
    "p_dates = parents_df.loc[trainset_new['parent_id']]['publish_date']\n",
    "\n",
    "# Convert to datetime\n",
    "c_dates = pd.to_datetime(c_dates, errors='coerce').dt.tz_localize(None)\n",
    "p_dates = pd.to_datetime(p_dates, errors='coerce').dt.tz_localize(None)\n",
    "\n",
    "# Calculate difference in days\n",
    "trainset_new['days_diff'] = (c_dates.values - p_dates.values) / np.timedelta64(1, 'D')\n",
    "trainset_new['days_diff'] = trainset_new['days_diff'].abs()\n",
    "\n",
    "# Create the binary feature (legacy logic was a window of 2 days)\n",
    "trainset_new['date_binary'] = (trainset_new['days_diff'] <= 2).astype(int)\n",
    "\n",
    "print(trainset_new.head())\n",
    "\n",
    "# Save the reconstructed set\n",
    "output_filename = 'trainset_reconstructed.csv'\n",
    "trainset_new.to_csv(output_filename, index=False)\n",
    "print(f\"Saved reconstructed training set to {output_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
