{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d4f849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cb4971",
   "metadata": {},
   "source": [
    "# File Stitcher - RUN THIS CODE ONCE!\n",
    "\n",
    "There are 350k files in the data folder, which takes forever for the computer to go through. Also, there is \"all parents\" csv but some of the individual parent files are not included. So for convenience and having a complete set we will first stitch all the raw data into a single file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b3f10a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for c_*.csv files.\n",
      "Found 174190 files. Merging them into full_children.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174190/174190 [04:30<00:00, 644.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Saved full_children.csv with 174190 rows.\n",
      "Loading existing all_parents.csv to merge.\n",
      "Found 1594 individual parent files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1594/1594 [00:02<00:00, 596.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Saved full_parents.csv. Total parents: 2656\n"
     ]
    }
   ],
   "source": [
    "# DATA DIR\n",
    "data_dir = Path(\"data\")\n",
    "\n",
    "def build_master_csv(file_pattern, output_name, id_column='id'):\n",
    "\n",
    "    print(f\"Looking for {file_pattern} files.\")\n",
    "    # Get list of all files matching the pattern\n",
    "    # For children, we exclude the '_output.csv' files which are legacy matches\n",
    "    all_files = glob.glob(str(data_dir / file_pattern))\n",
    "    \n",
    "    # Filter out 'output' files if we are looking for children\n",
    "    if 'c_' in file_pattern:\n",
    "        all_files = [f for f in all_files if '_output.csv' not in f]\n",
    "    \n",
    "    print(f\"Found {len(all_files)} files. Merging them into {output_name}.\")\n",
    "    \n",
    "    # Use a list to store dataframes (faster than appending to a dataframe)\n",
    "    df_list = []\n",
    "    \n",
    "    for filename in tqdm(all_files):\n",
    "        try:\n",
    "            # Read only the columns we need to save memory\n",
    "            # Adjust columns based on what your new_preprocess.ipynb needs\n",
    "            df = pd.read_csv(filename, index_col=None, header=0)\n",
    "            df_list.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {filename}: {e}\")\n",
    "            continue\n",
    "            \n",
    "    if df_list:\n",
    "        master_df = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "        \n",
    "        # Deduplicate just in case\n",
    "        if id_column in master_df.columns:\n",
    "            master_df = master_df.drop_duplicates(subset=[id_column])\n",
    "            master_df = master_df.set_index(id_column) # Set ID as index for faster lookups\n",
    "            \n",
    "        # Save to CSV\n",
    "        output_path = data_dir / output_name\n",
    "        master_df.to_csv(output_path)\n",
    "        print(f\"Success! Saved {output_name} with {len(master_df)} rows.\")\n",
    "        return master_df\n",
    "    else:\n",
    "        print(f\"No files found for {file_pattern}!\")\n",
    "        return None\n",
    "\n",
    "# Build full_children.csv (formerly childs_from_id.csv)\n",
    "build_master_csv(\"c_*.csv\", \"full_children.csv\")\n",
    "\n",
    "# Build full_parents.csv (merges individual p_ files AND the original all_parents.csv input)\n",
    "existing_parents = []\n",
    "if (data_dir / \"all_parents.csv\").exists():\n",
    "    print(\"Loading existing all_parents.csv to merge.\")\n",
    "    existing_parents.append(pd.read_csv(data_dir / \"all_parents.csv\"))\n",
    "\n",
    "# Get all p_*.csv files\n",
    "p_files = glob.glob(str(data_dir / \"p_*.csv\"))\n",
    "print(f\"Found {len(p_files)} individual parent files.\")\n",
    "\n",
    "for f in tqdm(p_files):\n",
    "    existing_parents.append(pd.read_csv(f))\n",
    "\n",
    "if existing_parents:\n",
    "    full_parents = pd.concat(existing_parents, axis=0, ignore_index=True)\n",
    "    # Deduplicate by ID\n",
    "    if 'id' in full_parents.columns:\n",
    "        full_parents = full_parents.drop_duplicates(subset=['id'])\n",
    "        full_parents = full_parents.set_index('id')\n",
    "    \n",
    "    # Save to NEW name\n",
    "    full_parents.to_csv(data_dir / \"full_parents.csv\")\n",
    "    print(f\"Success! Saved full_parents.csv. Total parents: {len(full_parents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baa3c74",
   "metadata": {},
   "source": [
    "# RUN THIS ONCE AS WELL !\n",
    "Same with this one, this cell aggregates all c_*_output.csv - s (realized later that I need them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "622a1cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 172387 match files. Stitching them into full_matches.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 172387/172387 [01:17<00:00, 2237.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Saved full_matches.csv with 861546 rows (including scores).\n"
     ]
    }
   ],
   "source": [
    "# DATA DIR\n",
    "data_dir = Path(\"data\")\n",
    "\n",
    "# Find all output files\n",
    "match_files = glob.glob(str(data_dir / \"*_output.csv\"))\n",
    "print(f\"Found {len(match_files)} match files. Stitching them into full_matches.csv.\")\n",
    "\n",
    "df_list = []\n",
    "\n",
    "# Loop through files\n",
    "for filename in tqdm(match_files):\n",
    "    try:\n",
    "        # We need the score to filter out low-quality matches later (Threshold > 88)\n",
    "        df = pd.read_csv(filename, usecols=['c', 'p', '%'], index_col=None, header=0)\n",
    "        df_list.append(df)\n",
    "    except ValueError:\n",
    "        # Handle case where file might be empty or missing columns\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "if df_list:\n",
    "    # Combine into one big dataframe\n",
    "    full_matches = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "    \n",
    "    # Rename columns to match our standard (c -> child_id, p -> parent_id)\n",
    "    full_matches = full_matches.rename(columns={'c': 'child_id', 'p': 'parent_id'})\n",
    "    \n",
    "    # Drop duplicates just in case\n",
    "    full_matches = full_matches.drop_duplicates()\n",
    "    \n",
    "    # Save to disk\n",
    "    output_path = data_dir / \"full_matches.csv\"\n",
    "    full_matches.to_csv(output_path, index=False)\n",
    "    print(f\"Success! Saved full_matches.csv with {len(full_matches)} rows (including scores).\")\n",
    "else:\n",
    "    print(\"No match files found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81270fe0",
   "metadata": {},
   "source": [
    "Carry on with rest of the processing:\n",
    "# SAFE TO RUN FROM THIS CELL AND DOWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47bce286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Model loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load the large Dutch model (For Model B)\n",
    "# If this crashes, Step 1 didn't work.\n",
    "nlp = spacy.load(\"nl_core_news_lg\", disable=['ner', 'parser'])\n",
    "print(\"Success: Model loaded!\")\n",
    "\n",
    "# Define paths\n",
    "data_path = Path(\"data\")\n",
    "\n",
    "# The English Stemmer for Model A (Make sure you imported nltk!)\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ee947a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download nl_core_news_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e455fa2c",
   "metadata": {},
   "source": [
    "Added Dutch SpaCy Lemmatization to PorterStemmer to actually capture Dutch meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9efb243b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dutch cleaning (Model B)\n",
    "def clean_text_dutch(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text) \n",
    "    doc = nlp(text)\n",
    "    # Keep lemmas (Dutch dictionary roots)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Legacy English cleaning (Model A)\n",
    "def clean_text_legacy(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = text.lower()\n",
    "\n",
    "    # Simple regex\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    stems = [ps.stem(t) for t in tokens]\n",
    "    return \" \".join(stems)\n",
    "\n",
    "# CALculating the similarity score:\n",
    "def jaccard_similarity(str1, str2):\n",
    "    \"\"\"Legacy metric: Intersection over Union\"\"\"\n",
    "    a = set(str1.split()) \n",
    "    b = set(str2.split())\n",
    "    if len(a) == 0 or len(b) == 0: return 0.0\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fa2fc1",
   "metadata": {},
   "source": [
    "Cleaning the dataframes before the matching loop so we don't clean the same text 10,000 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df0a94ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw CSVs, good luck to our laptops.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r5/_05_w2k14p728pkkf57ysjg40000gn/T/ipykernel_87929/2718462813.py:2: DtypeWarning: Columns (15,16,56,57,61,62,68,69,72) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  children_df = pd.read_csv(data_path / 'full_children.csv', index_col=0).fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded and cleaned.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading raw CSVs, good luck to our laptops.\")\n",
    "children_df = pd.read_csv(data_path / 'full_children.csv', index_col=0).fillna('')\n",
    "parents_df = pd.read_csv(data_path / 'full_parents.csv', index_col=0).fillna('')\n",
    "\n",
    "# Force indices to be integers to ensure matching works\n",
    "children_df.index = pd.to_numeric(children_df.index, errors='coerce').fillna(-1).astype(int)\n",
    "parents_df.index = pd.to_numeric(parents_df.index, errors='coerce').fillna(-1).astype(int)\n",
    "\n",
    "# Model B / Dutch\n",
    "children_df['clean_title_dutch'] = children_df['title'].astype(str).apply(clean_text_dutch)\n",
    "children_df['clean_content_dutch'] = children_df['content'].astype(str).apply(clean_text_dutch)\n",
    "parents_df['clean_title_dutch'] = parents_df['title'].astype(str).apply(clean_text_dutch)\n",
    "parents_df['clean_content_dutch'] = parents_df['content'].astype(str).apply(clean_text_dutch)\n",
    "\n",
    "# Model A / English\n",
    "children_df['clean_title_legacy'] = children_df['title'].astype(str).apply(clean_text_legacy)\n",
    "children_df['clean_content_legacy'] = children_df['content'].astype(str).apply(clean_text_legacy)\n",
    "parents_df['clean_title_legacy'] = parents_df['title'].astype(str).apply(clean_text_legacy)\n",
    "parents_df['clean_content_legacy'] = parents_df['content'].astype(str).apply(clean_text_legacy)\n",
    "\n",
    "print(\"Data loaded and cleaned.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a462b62",
   "metadata": {},
   "source": [
    "The old script had issues with label overwriting. I wrote this loop to explicitly extract matches from the 'related_children' column and generate exactly 1 random negative for every positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e853e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading matches.\n",
      "Loaded 861546 matches. Validated 391806 pairs existing in DB.\n",
      "Generating 1:1 pos:neg samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391806/391806 [00:05<00:00, 74243.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Generated 783612 rows (Balanced 50/50).\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading matches.\")\n",
    "try:\n",
    "    # Load the STITHCED file\n",
    "    matches_df = pd.read_csv(data_path / 'full_matches.csv')\n",
    "    \n",
    "    # Prevents crashes later when we try to look up text for scoring\n",
    "    valid_matches = matches_df[\n",
    "        (matches_df['child_id'].isin(children_df.index)) & \n",
    "        (matches_df['parent_id'].isin(parents_df.index))\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"Loaded {len(matches_df)} matches. Validated {len(valid_matches)} pairs existing in DB.\")\n",
    "    \n",
    "    training_rows = []\n",
    "    all_parent_ids = parents_df.index.tolist()\n",
    "    \n",
    "    print(\"Generating 1:1 pos:neg samples.\")\n",
    "    \n",
    "    # Iterate through the valid matches to build the training set\n",
    "    for _, row in tqdm(valid_matches.iterrows(), total=len(valid_matches)):\n",
    "        c_id = int(row['child_id'])\n",
    "        p_id = int(row['parent_id'])\n",
    "        \n",
    "        # Pos sampling\n",
    "        training_rows.append({\n",
    "            'child_id': c_id,\n",
    "            'parent_id': p_id,\n",
    "            'match': 1\n",
    "        })\n",
    "        \n",
    "        # egative sampling - pick a random parent that is NOT the match\n",
    "        while True:\n",
    "            random_pid = random.choice(all_parent_ids)\n",
    "            if random_pid != p_id:\n",
    "                break\n",
    "        \n",
    "        training_rows.append({\n",
    "            'child_id': c_id,\n",
    "            'parent_id': random_pid,\n",
    "            'match': 0\n",
    "        })\n",
    "\n",
    "    # Create final DataFrame\n",
    "    trainset_new = pd.DataFrame(training_rows)\n",
    "    print(f\"Success! Generated {len(trainset_new)} rows (Balanced 50/50).\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: full_matches.csv not found! Check firsts teps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d77f8df",
   "metadata": {},
   "source": [
    "Using nlp.pipe to vectorize everything in batches. This is much faster than .iterrows()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "404ccf66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# MOdel B\\n# Map IDs to Dutch text\\nc_titles_d = [children_df.at[cid, \\'clean_title_dutch\\'] if cid in children_df.index else \"\" for cid in trainset_new[\\'child_id\\']]\\np_titles_d = [parents_df.at[pid, \\'clean_title_dutch\\'] if pid in parents_df.index else \"\" for pid in trainset_new[\\'parent_id\\']]\\nc_content_d = [children_df.at[cid, \\'clean_content_dutch\\'] if cid in children_df.index else \"\" for cid in trainset_new[\\'child_id\\']]\\np_content_d = [parents_df.at[pid, \\'clean_content_dutch\\'] if pid in parents_df.index else \"\" for pid in trainset_new[\\'parent_id\\']]\\n\\nprint(\"Calculating Vectors.\")\\n\\n# Batch process with SpaCy\\nc_title_docs = list(nlp.pipe(c_titles_d, batch_size=200))\\np_title_docs = list(nlp.pipe(p_titles_d, batch_size=200))\\nc_cont_docs = list(nlp.pipe(c_content_d, batch_size=50))\\np_cont_docs = list(nlp.pipe(p_content_d, batch_size=50))\\n\\ntrainset_new[\\'title_sim_dutch\\'] = [c.similarity(p) if c.vector_norm and p.vector_norm else 0.0 for c, p in zip(c_title_docs, p_title_docs)]\\ntrainset_new[\\'content_sim_dutch\\'] = [c.similarity(p) if c.vector_norm and p.vector_norm else 0.0 for c, p in zip(c_cont_docs, p_cont_docs)]\\n\\n# For model A\\nprint(\"Calculating Legacy Jaccard.\")\\n\\n# Map IDs to Legacy text\\nc_titles_l = [children_df.at[cid, \\'clean_title_legacy\\'] if cid in children_df.index else \"\" for cid in trainset_new[\\'child_id\\']]\\np_titles_l = [parents_df.at[pid, \\'clean_title_legacy\\'] if pid in parents_df.index else \"\" for pid in trainset_new[\\'parent_id\\']]\\nc_content_l = [children_df.at[cid, \\'clean_content_legacy\\'] if cid in children_df.index else \"\" for cid in trainset_new[\\'child_id\\']]\\np_content_l = [parents_df.at[pid, \\'clean_content_legacy\\'] if pid in parents_df.index else \"\" for pid in trainset_new[\\'parent_id\\']]\\n\\n# Simple list comprehension for Jaccard\\ntrainset_new[\\'title_sim_legacy\\'] = [jaccard_similarity(c, p) for c, p in zip(c_titles_l, p_titles_l)]\\ntrainset_new[\\'content_sim_legacy\\'] = [jaccard_similarity(c, p) for c, p in zip(c_content_l, p_content_l)]\\n\\nprint(\"Scoring complete.\")\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# MOdel B\n",
    "# Map IDs to Dutch text\n",
    "c_titles_d = [children_df.at[cid, 'clean_title_dutch'] if cid in children_df.index else \"\" for cid in trainset_new['child_id']]\n",
    "p_titles_d = [parents_df.at[pid, 'clean_title_dutch'] if pid in parents_df.index else \"\" for pid in trainset_new['parent_id']]\n",
    "c_content_d = [children_df.at[cid, 'clean_content_dutch'] if cid in children_df.index else \"\" for cid in trainset_new['child_id']]\n",
    "p_content_d = [parents_df.at[pid, 'clean_content_dutch'] if pid in parents_df.index else \"\" for pid in trainset_new['parent_id']]\n",
    "\n",
    "print(\"Calculating Vectors.\")\n",
    "\n",
    "# Batch process with SpaCy\n",
    "c_title_docs = list(nlp.pipe(c_titles_d, batch_size=200))\n",
    "p_title_docs = list(nlp.pipe(p_titles_d, batch_size=200))\n",
    "c_cont_docs = list(nlp.pipe(c_content_d, batch_size=50))\n",
    "p_cont_docs = list(nlp.pipe(p_content_d, batch_size=50))\n",
    "\n",
    "trainset_new['title_sim_dutch'] = [c.similarity(p) if c.vector_norm and p.vector_norm else 0.0 for c, p in zip(c_title_docs, p_title_docs)]\n",
    "trainset_new['content_sim_dutch'] = [c.similarity(p) if c.vector_norm and p.vector_norm else 0.0 for c, p in zip(c_cont_docs, p_cont_docs)]\n",
    "\n",
    "# For model A\n",
    "print(\"Calculating Legacy Jaccard.\")\n",
    "\n",
    "# Map IDs to Legacy text\n",
    "c_titles_l = [children_df.at[cid, 'clean_title_legacy'] if cid in children_df.index else \"\" for cid in trainset_new['child_id']]\n",
    "p_titles_l = [parents_df.at[pid, 'clean_title_legacy'] if pid in parents_df.index else \"\" for pid in trainset_new['parent_id']]\n",
    "c_content_l = [children_df.at[cid, 'clean_content_legacy'] if cid in children_df.index else \"\" for cid in trainset_new['child_id']]\n",
    "p_content_l = [parents_df.at[pid, 'clean_content_legacy'] if pid in parents_df.index else \"\" for pid in trainset_new['parent_id']]\n",
    "\n",
    "# Simple list comprehension for Jaccard\n",
    "trainset_new['title_sim_legacy'] = [jaccard_similarity(c, p) for c, p in zip(c_titles_l, p_titles_l)]\n",
    "trainset_new['content_sim_legacy'] = [jaccard_similarity(c, p) for c, p in zip(c_content_l, p_content_l)]\n",
    "\n",
    "print(\"Scoring complete.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a823bd",
   "metadata": {},
   "source": [
    "Ram-safe version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36a22770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring 783612 rows in chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [9:17:29<00:00, 423.41s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Model A.\n",
      "Scoring Complete!\n"
     ]
    }
   ],
   "source": [
    "print(f\"Scoring {len(trainset_new)} rows in chunks.\")\n",
    "\n",
    "title_sims_dutch = []\n",
    "content_sims_dutch = []\n",
    "\n",
    "# Chunk size\n",
    "chunk_size = 10000\n",
    "\n",
    "# Loop through the dataset in chunks\n",
    "for start_idx in tqdm(range(0, len(trainset_new), chunk_size)):\n",
    "    end_idx = start_idx + chunk_size\n",
    "    batch_df = trainset_new.iloc[start_idx:end_idx]\n",
    "    \n",
    "    # Get text\n",
    "    c_titles = [children_df.at[cid, 'clean_title_dutch'] if cid in children_df.index else \"\" for cid in batch_df['child_id']]\n",
    "    p_titles = [parents_df.at[pid, 'clean_title_dutch'] if pid in parents_df.index else \"\" for pid in batch_df['parent_id']]\n",
    "    \n",
    "    c_contents = [children_df.at[cid, 'clean_content_dutch'] if cid in children_df.index else \"\" for cid in batch_df['child_id']]\n",
    "    p_contents = [parents_df.at[pid, 'clean_content_dutch'] if pid in parents_df.index else \"\" for pid in batch_df['parent_id']]\n",
    "\n",
    "    # Vectorize this batch\n",
    "    c_t_docs = list(nlp.pipe(c_titles, batch_size=200))\n",
    "    p_t_docs = list(nlp.pipe(p_titles, batch_size=200))\n",
    "    \n",
    "    c_c_docs = list(nlp.pipe(c_contents, batch_size=50))\n",
    "    p_c_docs = list(nlp.pipe(p_contents, batch_size=50))\n",
    "    \n",
    "    # Calculate Scores n Append to main lists\n",
    "    for c, p in zip(c_t_docs, p_t_docs):\n",
    "        if c.vector_norm and p.vector_norm:\n",
    "            title_sims_dutch.append(c.similarity(p))\n",
    "        else:\n",
    "            title_sims_dutch.append(0.0)\n",
    "            \n",
    "    for c, p in zip(c_c_docs, p_c_docs):\n",
    "        if c.vector_norm and p.vector_norm:\n",
    "            content_sims_dutch.append(c.similarity(p))\n",
    "        else:\n",
    "            content_sims_dutch.append(0.0)\n",
    "            \n",
    "trainset_new['title_sim_dutch'] = title_sims_dutch\n",
    "trainset_new['content_sim_dutch'] = content_sims_dutch\n",
    "\n",
    "# molde A\n",
    "print(\"Calculating Model A.\")\n",
    "\n",
    "# Prepare Legacy Text\n",
    "c_titles_l = [children_df.at[cid, 'clean_title_legacy'] if cid in children_df.index else \"\" for cid in trainset_new['child_id']]\n",
    "p_titles_l = [parents_df.at[pid, 'clean_title_legacy'] if pid in parents_df.index else \"\" for pid in trainset_new['parent_id']]\n",
    "c_content_l = [children_df.at[cid, 'clean_content_legacy'] if cid in children_df.index else \"\" for cid in trainset_new['child_id']]\n",
    "p_content_l = [parents_df.at[pid, 'clean_content_legacy'] if pid in parents_df.index else \"\" for pid in trainset_new['parent_id']]\n",
    "\n",
    "# Jaccard Function\n",
    "def jaccard_similarity(str1, str2):\n",
    "    a = set(str1.split()) \n",
    "    b = set(str2.split())\n",
    "    if len(a) == 0 or len(b) == 0: return 0.0\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "trainset_new['title_sim_legacy'] = [jaccard_similarity(c, p) for c, p in zip(c_titles_l, p_titles_l)]\n",
    "trainset_new['content_sim_legacy'] = [jaccard_similarity(c, p) for c, p in zip(c_content_l, p_content_l)]\n",
    "\n",
    "print(\"Scoring Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc41a593",
   "metadata": {},
   "source": [
    "Re-implementing the date binary check. I am omitting the complex taxonomy features for now as they require the legacy dictionary logic, but we have the core semantic features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dac9661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   child_id  parent_id  match  title_sim_dutch  content_sim_dutch  \\\n",
      "0    674464     674230      1         0.000000           0.891758   \n",
      "1    674464     728951      0         0.555879           0.899667   \n",
      "2    674464     674045      1         0.000000           0.891758   \n",
      "3    674464     672959      0        -0.064161          -0.016084   \n",
      "4    674464     673849      1         0.649244           0.704233   \n",
      "\n",
      "   title_sim_legacy  content_sim_legacy  days_diff  date_binary  \n",
      "0               0.0            0.085439   0.165046            1  \n",
      "1               0.0            0.047030  70.579225            0  \n",
      "2               0.0            0.085439   0.402882            1  \n",
      "3               0.0            0.012360        NaN            0  \n",
      "4               0.0            0.082879   0.435069            1  \n",
      "Saved reconstructed training set to trainset_reconstructed.csv\n"
     ]
    }
   ],
   "source": [
    "# Add Date Logic\n",
    "# (We need to look up dates from the original dfs)\n",
    "c_dates = children_df.loc[trainset_new['child_id']]['publish_date']\n",
    "p_dates = parents_df.loc[trainset_new['parent_id']]['publish_date']\n",
    "\n",
    "# Convert to datetime\n",
    "c_dates = pd.to_datetime(c_dates, errors='coerce').dt.tz_localize(None)\n",
    "p_dates = pd.to_datetime(p_dates, errors='coerce').dt.tz_localize(None)\n",
    "\n",
    "# Calculate difference in days\n",
    "trainset_new['days_diff'] = (c_dates.values - p_dates.values) / np.timedelta64(1, 'D')\n",
    "trainset_new['days_diff'] = trainset_new['days_diff'].abs()\n",
    "\n",
    "# Create the binary feature (legacy logic was a window of 2 days)\n",
    "trainset_new['date_binary'] = (trainset_new['days_diff'] <= 2).astype(int)\n",
    "\n",
    "print(trainset_new.head())\n",
    "\n",
    "# Save the reconstructed set\n",
    "output_filename = 'trainset_reconstructed.csv'\n",
    "trainset_new.to_csv(output_filename, index=False)\n",
    "print(f\"Saved reconstructed training set to {output_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
